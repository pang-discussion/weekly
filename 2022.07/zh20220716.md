## 周报概要20220703
> 这两周主要学习了text to image相关的知识，在Autoencoder中简单复习了无监督学习中的降维方法（如PCA），了解了最近在生成方面表现出色的diffusion model。
> 
> 主要看了两篇CVPR2022的text to image生成的文章，其中第一篇是在第二篇的基础上做出来的，在arXiv上历时1~2年之后被收录。
---

### 1. CVPR-2022-Text to Image Generation with Semantic-Spatial Aware GAN
* 虽然在GAN最新进展的基础之上，现有的text to image generation model仍然具有局限，表现为：① CBN（conditional batch normalization）的方法在整个image feature上均等的作用在每个特征维度之上，没有对局部特征进行细粒度分析。② text encoder在训练过程中一直是fix的，它应该随着图像的生成而不断更新来学到更好的文本特征表示。
* 为了解决上述局限，作者提出了一个novel framework（语义空间感知的生成对抗网络，Semantic-Spatial Aware GAN），两个特点：① end to end fashion；② 提出了一个语义-空间感知卷积网络（Semantic-Spatial Aware Convolution Network）。
* 完整的阅读笔记可以点击[此处](https://docs.qq.com/doc/p/d40f17e15f18299673a71281a6211d61d2508e8f?dver=3.0.0)查看。

---

### 2. CVPR-2022 DF-GAN A Simple and Effective Baseline for Text-to-Image Synthesis
* 现存的T2I task一般采用的是stacked结构，连接多个GD实现高质量图像的生成，但是这种方法有三个缺陷：① stacked的结构引入了不同image scale的generator间的纠缠，比如下图中的shape。② 选择将text embedding network fix住，这样会限制文本对图像生成的监督能力。③ 采用cross-modal attention-based text-image fusion，因为其计算代价太大只能被限制使用在固定的64x64和128x128这两个尺寸的image feature上。
* 提出了简单高效的Deep Fusion Generative Adversarial Networks (DF-GAN)，包括三个亮点：① a novel one-stage text-to-image backbone，避免了之前stacked结构不同generator之间的纠缠。② a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output，在不引入额外网络的情况下，强化了文本图像之间语义的一致性。③ a novel deep text-image fusion block，实现文本和图像的深层次语义融合。
* 强调DFGAN**简单但是高效**。
* 完整的阅读笔记可以点击[此处](https://docs.qq.com/doc/p/fc1da62a4af232b2c20145186e6b0d2a9e2acabc?dver=3.0.0)查看。

